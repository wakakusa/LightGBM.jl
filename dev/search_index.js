var documenterSearchIndex = {"docs":
[{"location":"#LightGBM.jl-1","page":"Home","title":"LightGBM.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Modules = [LightGBM]","category":"page"},{"location":"#LightGBM.LGBMBinary-Tuple{}","page":"Home","title":"LightGBM.LGBMBinary","text":"LGBMBinary(; [num_iterations = 10,\n              learning_rate = .1,\n              num_leaves = 127,\n              max_depth = -1,\n              tree_learner = \"serial\",\n              num_threads = Sys.CPU_THREADS,\n              histogram_pool_size = -1.,\n              min_data_in_leaf = 100,\n              min_sum_hessian_in_leaf = 10.,\n              lambda_l1 = 0.,\n              lambda_l2 = 0.,\n              min_gain_to_split = 0.,\n              feature_fraction = 1.,\n              feature_fraction_seed = 2,\n              bagging_fraction = 1.,\n              bagging_freq = 0,\n              bagging_seed = 3,\n              early_stopping_round = 0,\n              max_bin = 255,\n              data_random_seed = 1,\n              init_score = \"\",\n              is_sparse = true,\n              save_binary = false,\n              categorical_feature = Int[],\n              sigmoid = 1.,\n              is_unbalance = false,\n              metric = [\"binary_logloss\"],\n              metric_freq = 1,\n              is_training_metric = false,\n              ndcg_at = Int[],\n              num_machines = 1,\n              local_listen_port = 12400,\n              time_out = 120,\n              machine_list_file = \"\",\n              device_type=\"cpu\",\n              random_seed =1])\n\nReturn a LGBMBinary estimator.\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.LGBMMulticlass-Tuple{}","page":"Home","title":"LightGBM.LGBMMulticlass","text":"LGBMMulticlass(; [num_iterations = 10,\n                  learning_rate = .1,\n                  num_leaves = 127,\n                  max_depth = -1,\n                  tree_learner = \"serial\",\n                  num_threads = Sys.CPU_THREADS,\n                  histogram_pool_size = -1.,\n                  min_data_in_leaf = 100,\n                  min_sum_hessian_in_leaf = 10.,\n                  lambda_l1 = 0.,\n                  lambda_l2 = 0.,\n                  min_gain_to_split = 0.,\n                  feature_fraction = 1.,\n                  feature_fraction_seed = 2,\n                  bagging_fraction = 1.,\n                  bagging_freq = 0,\n                  bagging_seed = 3,\n                  early_stopping_round = 0,\n                  max_bin = 255,\n                  data_random_seed = 1,\n                  init_score = \"\",\n                  is_sparse = true,\n                  save_binary = false,\n                  categorical_feature = Int[],\n                  is_unbalance = false,\n                  metric = [\"multi_logloss\"],\n                  metric_freq = 1,\n                  is_training_metric = false,\n                  ndcg_at = Int[],\n                  num_machines = 1,\n                  local_listen_port = 12400,\n                  time_out = 120,\n                  machine_list_file = \"\",\n                  num_class = 1,\n                  device_type=\"cpu\",\n                  random_seed=1])\n\nReturn a LGBMMulticlass estimator.\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.LGBMRegression-Tuple{}","page":"Home","title":"LightGBM.LGBMRegression","text":"LGBMRegression(; [num_iterations = 10,\n                  learning_rate = .1,\n                  num_leaves = 127,\n                  max_depth = -1,\n                  tree_learner = \"serial\",\n                  num_threads = Sys.CPU_THREADS,\n                  histogram_pool_size = -1.,\n                  min_data_in_leaf = 100,\n                  min_sum_hessian_in_leaf = 10.,\n                  lambda_l1 = 0.,\n                  lambda_l2 = 0.,\n                  min_gain_to_split = 0.,\n                  feature_fraction = 1.,\n                  feature_fraction_seed = 2,\n                  bagging_fraction = 1.,\n                  bagging_freq = 0,\n                  bagging_seed = 3,\n                  early_stopping_round = 0,\n                  max_bin = 255,\n                  data_random_seed = 1,\n                  init_score = \"\",\n                  is_sparse = true,\n                  save_binary = false,\n                  categorical_feature = Int[],\n                  is_unbalance = false,\n                  metric = [\"l2\"],\n                  metric_freq = 1,\n                  is_training_metric = false,\n                  ndcg_at = Int[],\n                  num_machines = 1,\n                  local_listen_port = 12400,\n                  time_out = 120,\n                  machine_list_file = \"\",\n                 device_type=\"cpu\",\n                 random_seed = 1])\n\nReturn a LGBMRegression estimator.\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator,Array{TX,2},Array{Ty,1},Any}} where Ty<:Real where TX<:Real","page":"Home","title":"LightGBM.cv","text":"cv(estimator, X, y, splits; [verbosity = 1])\n\nCross-validate the estimator with features data X and label y. The iterable splits provides vectors of indices for the training dataset. The remaining indices are used to create the validation dataset.\n\nReturn a dictionary with an entry for the validation dataset and, if the parameter is_training_metric is set in the estimator, an entry for the training dataset. Each entry of the dictionary is another dictionary with an entry for each validation metric in the estimator. Each of these entries is an array that holds the validation metric's value for each dataset, at the last valid iteration.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to be fit.\nX::Matrix{TX<:Real}: the features data.\ny::Vector{Ty<:Real}: the labels.\nsplits: the iterable providing arrays of indices for the training dataset.\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.fit-Union{Tuple{Ti}, Tuple{Tw}, Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator,Array{TX,2},Array{Ty,1},Vararg{Tuple{Array{TX,2},Array{Ty,1}},N} where N}} where Ti<:Real where Tw<:Real where Ty<:Real where TX<:Real","page":"Home","title":"LightGBM.fit","text":"fit(estimator, X, y[, test...]; [verbosity = 1, is_row_major = false])\n\nFit the estimator with features data X and label y using the X-y pairs in test as validation sets.\n\nReturn a dictionary with an entry for each validation set. Each entry of the dictionary is another dictionary with an entry for each validation metric in the estimator. Each of these entries is an array that holds the validation metric's value at each iteration.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to be fit.\nX::Matrix{TX<:Real}: the features data.\ny::Vector{Ty<:Real}: the labels.\ntest::Tuple{Matrix{TX},Vector{Ty}}...: optionally contains one or more tuples of X-y pairs of   the same types as X and y that should be used as validation sets.\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\nis_row_major::Bool: keyword argument that indicates whether or not X is row-major. true   indicates that it is row-major, false indicates that it is column-major (Julia's default).\nweights::Vector{Tw<:Real}: the training weights.\ninit_score::Vector{Ti<:Real}: the init scores.\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.formattedclassfit-Tuple{Array,Array}","page":"Home","title":"LightGBM.formattedclassfit","text":"formattedclassfit(result::Array,Xtest::Array)  \n\n予測精度の一番高い結果だけをLightGBM実行形式で出力される形式で出力  \n\nOnly the result with the highest prediction accuracy is output in the output format in the LightGBM execution format  \n\nArguments\n\nresult::Array:prediction result.   Xtest::Array:the features data.  \n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.loadmodel-Tuple{LGBMEstimator,String}","page":"Home","title":"LightGBM.loadmodel","text":"loadmodel(estimator, filename)\n\nLoad the fitted model filename into estimator. Note that this only loads the fitted model—not the parameters or data of the estimator whose model was saved as filename.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to use in the prediction.\nfilename::String: the name of the file that contains the model.\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.metaformattedclassresult-Tuple{Array,Array}","page":"Home","title":"LightGBM.metaformattedclassresult","text":"metaformattedclassresult(result::Array,Xtest::Array)  \n\nLightGBM実行形式で出力される行列フォーマットに変換  \n\nConverts to matrix format output in LightGBM executable format  \n\nArguments\n\nresult::Array:prediction   Xtest::Array:the features data.  \n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.metaformattedclassresult-Tuple{Array{T,2} where T}","page":"Home","title":"LightGBM.metaformattedclassresult","text":"metaformattedclassresult(metaformattedresult::Array)  \n\n分類予測結果から、予測精度の一番高い結果だけをLightGBM実行形式で出力される形式で出力  \n\nFrom the classification prediction result, only the result with the highest prediction accuracy is output in the output format in the LightGBM execution format  \n\nArguments\n\nmetaformattedresult::Array:formatted prediction result.  \n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.predict-Union{Tuple{TX}, Tuple{LGBMEstimator,Array{TX,2}}} where TX<:Real","page":"Home","title":"LightGBM.predict","text":"predict(estimator, X; [predict_type = 0, num_iterations = -1, verbosity = 1,\nis_row_major = false])\n\nReturn an array with the labels that the estimator predicts for features data X.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to use in the prediction.\nX::Matrix{T<:Real}: the features data.\npredict_type::Integer: keyword argument that controls the prediction type. 0 for normal   scores with transform (if needed), 1 for raw scores, 2 for leaf indices.\nnum_iterations::Integer: keyword argument that sets the number of iterations of the model to   use in the prediction. < 0 for all iterations.\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\nis_row_major::Bool: keyword argument that indicates whether or not X is row-major. true   indicates that it is row-major, false indicates that it is column-major (Julia's default).\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.predict2-Union{Tuple{TX}, Tuple{LGBMEstimator,Array{TX,2}}} where TX<:Real","page":"Home","title":"LightGBM.predict2","text":"predict2(estimator::LGBMEstimator, Xtest::Array)  \n\n分類予測の場合、予測精度の一番高い結果だけを出力する。その他の予測は結果をそのまま出力  \n\nIn the case of classification prediction, only the result with the highest prediction accuracy is output. Other predictions output the results as they are  \n\nArguments\n\nestimator::LGBMEstimator: the estimator to use in the prediction.\nX::Matrix{T<:Real}: the features data.\npredict_type::Integer: keyword argument that controls the prediction type. 0 for normal   scores with transform (if needed), 1 for raw scores, 2 for leaf indices.\nnum_iterations::Integer: keyword argument that sets the number of iterations of the model to   use in the prediction. < 0 for all iterations.\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\nis_row_major::Bool: keyword argument that indicates whether or not X is row-major. true   indicates that it is row-major, false indicates that it is column-major (Julia's default).\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.savemodel-Tuple{LGBMEstimator,String}","page":"Home","title":"LightGBM.savemodel","text":"savemodel(estimator, filename; [num_iteration = -1])\n\nSave the fitted model in estimator as filename.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to use in the prediction.\nfilename::String: the name of the file to save the model in.\nnum_iteration::Integer: keyword argument that sets the number of iterations of the model that   should be saved. < 0 for all iterations.\n\n\n\n\n\n","category":"method"},{"location":"#LightGBM.search_cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator,Array{TX,2},Array{Ty,1},Any,Any}} where Ty<:Real where TX<:Real","page":"Home","title":"LightGBM.search_cv","text":"search_cv(estimator, X, y, splits, params; [verbosity = 1])\n\nExhaustive search over the specified sets of parameter values for the estimator with features data X and label y. The iterable splits provides vectors of indices for the training dataset. The remaining indices are used to create the validation dataset.\n\nReturn an array with a tuple for each set of parameters value, where the first entry is a set of parameter values and the second entry the cross-validation outcome of those values. This outcome is a dictionary with an entry for the validation dataset and, if the parameter is_training_metric is set in the estimator, an entry for the training dataset. Each entry of the dictionary is another dictionary with an entry for each validation metric in the estimator. Each of these entries is an array that holds the validation metric's value for each dataset, at the last valid iteration.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to be fit.\nX::Matrix{TX<:Real}: the features data.\ny::Vector{Ty<:Real}: the labels.\nsplits: the iterable providing arrays of indices for the training dataset.\nparams: the iterable providing dictionaries of pairs of parameters (Symbols) and values to   configure the estimator with.\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\n\n\n\n\n\n","category":"method"}]
}
